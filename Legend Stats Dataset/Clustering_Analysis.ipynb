{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define legend categories\n",
    "legend_categories = {\n",
    "    'Assault': ['Bangalore', 'Fuse', 'Ash', 'Mad Maggie', 'Ballistic'],\n",
    "    'Skirmisher': ['Pathfinder', 'Wraith', 'Octane', 'Revenant', 'Horizon', 'Valkyrie', 'Alter'],\n",
    "    'Recon': ['Bloodhound', 'Crypto', 'Seer', 'Vantage'],\n",
    "    'Support': ['Gibraltar', 'Lifeline', 'Mirage', 'Loba', 'Newcastle', 'Conduit'],\n",
    "    'Controller': ['Caustic', 'Wattson', 'Rampart', 'Catalyst']\n",
    "}\n",
    "legend_to_category = {legend: role for role, legends in legend_categories.items() for legend in legends}\n",
    "legends = list(legend_to_category.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_to_category = {legend: role for role, legends in legend_categories.items() for legend in legends}\n",
    "legends = list(legend_to_category.keys())\n",
    "\n",
    "base_path = r\"\"\n",
    "damage_path = os.path.join(base_path, 'Legend Damage')\n",
    "kills_path = os.path.join(base_path, 'Legend Kills')\n",
    "matches_path = os.path.join(base_path, 'Legend Matches Played')\n",
    "wins_path = os.path.join(base_path, 'Legend Wins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Read and clean data ---\n",
    "legend_dataframes = []\n",
    "for legend in legends:\n",
    "    try:\n",
    "        legend_damage_file = os.path.join(damage_path, f\"{legend}_damage.csv\")\n",
    "        legend_kills_file = os.path.join(kills_path, f\"{legend}_kills.csv\")\n",
    "        legend_matches_file = os.path.join(matches_path, f\"{legend}_games_played.csv\")\n",
    "        legend_wins_file = os.path.join(wins_path, f\"{legend}_wins.csv\")\n",
    "        \n",
    "        required_files = [legend_damage_file, legend_kills_file, legend_matches_file, legend_wins_file]\n",
    "        missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "        if missing_files:\n",
    "            print(f\"Data files for legend '{legend}' are missing: {missing_files}. Skipping this legend.\")\n",
    "            continue\n",
    "        \n",
    "        # Read and clean each file\n",
    "        def clean_numeric_column(file_path, col_name):\n",
    "            df = pd.read_csv(file_path, header=None, names=[col_name], skiprows=1)\n",
    "            df[col_name] = df[col_name].astype(str).str.replace(',', '').str.replace('\"', '').str.strip()\n",
    "            df[col_name] = pd.to_numeric(df[col_name], errors='coerce')\n",
    "            return df\n",
    "\n",
    "        df_damage = clean_numeric_column(legend_damage_file, 'Damage')\n",
    "        df_kills = clean_numeric_column(legend_kills_file, 'Kills')\n",
    "        df_matches = clean_numeric_column(legend_matches_file, 'Games Played')\n",
    "        df_wins = clean_numeric_column(legend_wins_file, 'Wins')\n",
    "\n",
    "        # Combine by index\n",
    "        df_legend = pd.concat([df_damage.reset_index(drop=True),\n",
    "                               df_kills.reset_index(drop=True),\n",
    "                               df_matches.reset_index(drop=True),\n",
    "                               df_wins.reset_index(drop=True)], axis=1)\n",
    "        df_legend['legend_name'] = legend\n",
    "        legend_dataframes.append(df_legend)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing legend '{legend}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Combine and preprocess ---\n",
    "all_legends_df = pd.concat(legend_dataframes, ignore_index=True)\n",
    "all_legends_df['Legend_Category'] = all_legends_df['legend_name'].map(legend_to_category)\n",
    "\n",
    "# Remove legends with missing Wins data\n",
    "legends_missing_wins = ['Ballistic', 'Conduit', 'Alter', 'Newcastle']\n",
    "all_legends_df = all_legends_df[~all_legends_df['legend_name'].isin(legends_missing_wins)]\n",
    "\n",
    "# Ensure numeric columns\n",
    "for col in ['Kills', 'Wins', 'Games Played', 'Damage']:\n",
    "    all_legends_df[col] = pd.to_numeric(all_legends_df[col], errors='coerce')\n",
    "\n",
    "all_legends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that columns are numeric, forcing invalid values to NaN\n",
    "all_legends_df['Kills'] = pd.to_numeric(all_legends_df['Kills'], errors='coerce')\n",
    "all_legends_df['Wins'] = pd.to_numeric(all_legends_df['Wins'], errors='coerce')\n",
    "all_legends_df['Games Played'] = pd.to_numeric(all_legends_df['Games Played'], errors='coerce')\n",
    "all_legends_df['Damage'] = pd.to_numeric(all_legends_df['Damage'], errors='coerce')\n",
    "\n",
    "# Avoid division by zero and handle NaN\n",
    "all_legends_df['Kills_per_Win'] = all_legends_df.apply(lambda row: row['Kills'] / row['Wins'] if pd.notna(row['Wins']) and row['Wins'] > 0 else 0, axis=1)\n",
    "all_legends_df['Kills_per_Match'] = all_legends_df.apply(lambda row: row['Kills'] / row['Games Played'] if pd.notna(row['Games Played']) and row['Games Played'] > 0 else 0, axis=1)\n",
    "all_legends_df['Damage_per_Match'] = all_legends_df.apply(lambda row: row['Damage'] / row['Games Played'] if pd.notna(row['Games Played']) and row['Games Played'] > 0 else 0, axis=1)\n",
    "\n",
    "# Group by 'Legend_Category' and calculate the mean of the relevant columns\n",
    "averaged_stats_df = all_legends_df.groupby('Legend_Category').agg(\n",
    "    Average_Kills_per_Win=('Kills_per_Win', 'mean'),\n",
    "    Average_Kills_per_Match=('Kills_per_Match', 'mean'),\n",
    "    Average_Damage_per_Match=('Damage_per_Match', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated statistics\n",
    "print(averaged_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of legends with missing Wins data\n",
    "legends_missing_wins = ['Ballistic', 'Conduit', 'Alter', 'Newcastle']\n",
    "\n",
    "# Filter out these legends from the main DataFrame\n",
    "filtered_legends_df = all_legends_df[~all_legends_df['legend_name'].isin(legends_missing_wins)]\n",
    "\n",
    "# Verify the exclusion\n",
    "print(filtered_legends_df['legend_name'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_legends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Data Cleaning\n",
    "# Exclude incomplete data and remove outliers based only on selected metrics\n",
    "filtered_legends_df = filtered_legends_df.dropna(subset=['Kills_per_Match', 'Kills_per_Win'])\n",
    "\n",
    "# Handle Outliers using Z-score (for only Kills_per_Match and Kills_per_Win)\n",
    "z_scores = np.abs(stats.zscore(filtered_legends_df[['Kills_per_Match', 'Kills_per_Win']]))\n",
    "filtered_legends_df = filtered_legends_df[(z_scores < 3).all(axis=1)]\n",
    "\n",
    "# 2. Feature Selection and Scaling\n",
    "# Only include Kills_per_Match and Kills_per_Win for clustering\n",
    "features = filtered_legends_df[['Kills_per_Match', 'Kills_per_Win']]\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# 3. PCA for Visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "pca_df['Legend_Category'] = filtered_legends_df['Legend_Category']\n",
    "\n",
    "# 4. Determine Optimal K using Elbow and Silhouette\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "for i in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 11), wcss, marker='o')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.xticks(range(2, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 11), silhouette_scores, marker='o')\n",
    "plt.title('Silhouette Analysis for Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(range(2, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose K=3 based on previous analysis\n",
    "optimal_k = 5\n",
    "\n",
    "# 5. Implement K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42)\n",
    "cluster_labels_km = kmeans.fit_predict(scaled_features)\n",
    "filtered_legends_df['Cluster_KM'] = cluster_labels_km\n",
    "\n",
    "# 6. Implement Gaussian Mixture Model (GMM)\n",
    "gmm = GaussianMixture(n_components=optimal_k, covariance_type='full', random_state=42)\n",
    "cluster_labels_gmm = gmm.fit_predict(scaled_features)\n",
    "filtered_legends_df['Cluster_GMM'] = cluster_labels_gmm\n",
    "\n",
    "# 7. Implement Hierarchical Clustering\n",
    "hc = AgglomerativeClustering(n_clusters=optimal_k, metric='euclidean', linkage='ward')\n",
    "cluster_labels_hc = hc.fit_predict(scaled_features)\n",
    "filtered_legends_df['Cluster_HC'] = cluster_labels_hc\n",
    "\n",
    "# 8. Evaluate Clustering Results\n",
    "\n",
    "# K-Means Evaluation\n",
    "silhouette_km = silhouette_score(scaled_features, cluster_labels_km)\n",
    "ch_km = calinski_harabasz_score(scaled_features, cluster_labels_km)\n",
    "ari_km = adjusted_rand_score(filtered_legends_df['Legend_Category'], cluster_labels_km)\n",
    "\n",
    "print(f'K-Means Clustering - Silhouette Score: {silhouette_km}')\n",
    "print(f'K-Means Clustering - Calinski-Harabasz Index: {ch_km}')\n",
    "print(f'K-Means Clustering - Adjusted Rand Index: {ari_km}')\n",
    "\n",
    "# GMM Evaluation\n",
    "silhouette_gmm = silhouette_score(scaled_features, cluster_labels_gmm)\n",
    "ch_gmm = calinski_harabasz_score(scaled_features, cluster_labels_gmm)\n",
    "ari_gmm = adjusted_rand_score(filtered_legends_df['Legend_Category'], cluster_labels_gmm)\n",
    "\n",
    "print(f'GMM Clustering - Silhouette Score: {silhouette_gmm}')\n",
    "print(f'GMM Clustering - Calinski-Harabasz Index: {ch_gmm}')\n",
    "print(f'GMM Clustering - Adjusted Rand Index: {ari_gmm}')\n",
    "\n",
    "# Hierarchical Clustering Evaluation\n",
    "silhouette_hc = silhouette_score(scaled_features, cluster_labels_hc)\n",
    "ch_hc = calinski_harabasz_score(scaled_features, cluster_labels_hc)\n",
    "ari_hc = adjusted_rand_score(filtered_legends_df['Legend_Category'], cluster_labels_hc)\n",
    "\n",
    "print(f'Hierarchical Clustering - Silhouette Score: {silhouette_hc}')\n",
    "print(f'Hierarchical Clustering - Calinski-Harabasz Index: {ch_hc}')\n",
    "print(f'Hierarchical Clustering - Adjusted Rand Index: {ari_hc}')\n",
    "\n",
    "# 9. Visualize Clustering Results (K-Means as Example)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=principal_components[:,0], y=principal_components[:,1], hue=cluster_labels_km, palette='Set1', s=100)\n",
    "plt.title('K-Means Clustering Results with PCA-Reduced Features (KM)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Clustering Results (Hierarchical Clustering)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=principal_components[:,0], y=principal_components[:,1], hue=cluster_labels_hc, palette='Set1', s=100)\n",
    "plt.title('Hierarchical Clustering Results with PCA-Reduced Features (HC)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Principal Component 1', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Principal Component 2', fontsize=14, fontweight='bold')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Clustering Results (GMM)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=principal_components[:,0], y=principal_components[:,1], hue=cluster_labels_gmm, palette='Set1', s=100)\n",
    "plt.title('GMM Clustering Results with PCA-Reduced Features (GMM)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K means clustering\n",
    "cluster_profiles_km = filtered_legends_df.groupby('Cluster_KM').agg(\n",
    "    Mean_Kills_per_Match=('Kills_per_Match', 'mean'),\n",
    "    Mean_Kills_per_Win=('Kills_per_Win', 'mean'),\n",
    ").reset_index()\n",
    "print(\"cluster_profiles_km\")\n",
    "print(cluster_profiles_km)\n",
    "#HC clustering\n",
    "cluster_profiles_hc = filtered_legends_df.groupby('Cluster_HC').agg(\n",
    "    Mean_Kills_per_Match=('Kills_per_Match', 'mean'),\n",
    "    Mean_Kills_per_Win=('Kills_per_Win', 'mean'),\n",
    ").reset_index()\n",
    "print(\"cluster_profiles_hc\")\n",
    "print(cluster_profiles_hc)\n",
    "#GMM clustering\n",
    "cluster_profiles_gmm = filtered_legends_df.groupby('Cluster_GMM').agg(\n",
    "    Mean_Kills_per_Match=('Kills_per_Match', 'mean'),\n",
    "    Mean_Kills_per_Win=('Kills_per_Win', 'mean'),\n",
    ").reset_index()\n",
    "print(\"cluster_profiles_gmm\")\n",
    "print(cluster_profiles_gmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def cluster_composition_analysis(df, cluster_label, legend_label, method_name):\n",
    "    \"\"\"\n",
    "    Generates cross-tabulation, percentage distribution, and visualization for cluster composition.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing cluster labels and legend categories.\n",
    "    - cluster_label: string, column name for cluster labels.\n",
    "    - legend_label: string, column name for legend categories.\n",
    "    - method_name: string, name of the clustering method for titles.\n",
    "    \"\"\"\n",
    "    # Cross-tabulation\n",
    "    crosstab = pd.crosstab(df[cluster_label], df[legend_label], margins=True)\n",
    "    print(f\"\\n{method_name} - Cluster Composition:\")\n",
    "    print(crosstab)\n",
    "    \n",
    "    # Percentage distribution\n",
    "    crosstab_percent = pd.crosstab(df[cluster_label], df[legend_label], normalize='index') * 100\n",
    "    print(f\"\\n{method_name} - Cluster Composition (%):\")\n",
    "    print(crosstab_percent.round(2))\n",
    "    \n",
    "    # Visualization\n",
    "    crosstab_percent.plot(kind='bar', stacked=True, figsize=(10,7), colormap='Set3')\n",
    "    plt.title(f'{method_name} - Legend Category Distribution per Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.legend(title='Legend Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming 'filtered_legends_df' is your DataFrame\n",
    "\n",
    "# K-Means Clustering\n",
    "cluster_composition_analysis(\n",
    "    df=filtered_legends_df,\n",
    "    cluster_label='Cluster_KM',\n",
    "    legend_label='Legend_Category',\n",
    "    method_name='K-Means Clustering'\n",
    ")\n",
    "\n",
    "# GMM Clustering\n",
    "cluster_composition_analysis(\n",
    "    df=filtered_legends_df,\n",
    "    cluster_label='Cluster_GMM',\n",
    "    legend_label='Legend_Category',\n",
    "    method_name='GMM Clustering'\n",
    ")\n",
    "\n",
    "# Hierarchical Clustering\n",
    "cluster_composition_analysis(\n",
    "    df=filtered_legends_df,\n",
    "    cluster_label='Cluster_HC',\n",
    "    legend_label='Legend_Category',\n",
    "    method_name='Hierarchical Clustering'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define colors for the categories using specified RGB values\n",
    "category_colors = {\n",
    "    'Assault': '#D04841',       # rgb(208,72,65)\n",
    "    'Skirmisher': '#BCAB36',    # rgb(188,171,54)\n",
    "    'Recon': '#8A3CE0',         # rgb(138,60,224)\n",
    "    'Controller': '#71C84E',    # rgb(113,200,78)\n",
    "    'Support': '#4274DC'        # rgb(66,116,220)\n",
    "}\n",
    "\n",
    "def cluster_composition_analysis(df, cluster_label, legend_label, method_name):\n",
    "    \"\"\"\n",
    "    Generates cross-tabulation, percentage distribution, and visualization for cluster composition.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame containing cluster labels and legend categories.\n",
    "    - cluster_label: string, column name for cluster labels.\n",
    "    - legend_label: string, column name for legend categories.\n",
    "    - method_name: string, name of the clustering method for titles.\n",
    "    \"\"\"\n",
    "    # Cross-tabulation\n",
    "    crosstab = pd.crosstab(df[cluster_label], df[legend_label], margins=True)\n",
    "    print(f\"\\n{method_name} - Cluster Composition:\")\n",
    "    print(crosstab)\n",
    "    \n",
    "    # Percentage distribution\n",
    "    crosstab_percent = pd.crosstab(df[cluster_label], df[legend_label], normalize='index') * 100\n",
    "    print(f\"\\n{method_name} - Cluster Composition (%):\")\n",
    "    print(crosstab_percent.round(2))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # Assign colors based on legend categories\n",
    "    color_list = [category_colors.get(cat, 'grey') for cat in crosstab_percent.columns]\n",
    "    \n",
    "    crosstab_percent.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        color=color_list,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    plt.title(f'{method_name} - Legend Category Distribution per Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.legend(title='Legend Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming 'filtered_legends_df' is your DataFrame\n",
    "\n",
    "# K-Means Clustering\n",
    "cluster_composition_analysis(\n",
    "    df=filtered_legends_df,\n",
    "    cluster_label='Cluster_KM',\n",
    "    legend_label='Legend_Category',\n",
    "    method_name='K-Means Clustering'\n",
    ")\n",
    "\n",
    "# GMM Clustering\n",
    "cluster_composition_analysis(\n",
    "    df=filtered_legends_df,\n",
    "    cluster_label='Cluster_GMM',\n",
    "    legend_label='Legend_Category',\n",
    "    method_name='GMM Clustering'\n",
    ")\n",
    "\n",
    "# Hierarchical Clustering\n",
    "cluster_composition_analysis(\n",
    "    df=filtered_legends_df,\n",
    "    cluster_label='Cluster_HC',\n",
    "    legend_label='Legend_Category',\n",
    "    method_name='Hierarchical Clustering'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the horizontal bar chart - Kills per Match\n",
    "pairs_kills_per_match = [\n",
    "    \"Assault vs Recon\",\n",
    "    \"Assault vs Support\",\n",
    "    \"Assault vs Skirmisher\",\n",
    "    \"Recon vs Support\"\n",
    "]\n",
    "effect_sizes_kills_per_match = [0.333, 0.325, 0.279, -0.265]\n",
    "\n",
    "# Data for the horizontal bar chart - Kills per Win\n",
    "pairs_kills_per_win = [\n",
    "    \"Assault vs Support\",\n",
    "    \"Support vs Controller\",\n",
    "    \"Recon vs Support\"\n",
    "]\n",
    "effect_sizes_kills_per_win = [0.753, -0.786, 0.681]\n",
    "\n",
    "# Create the horizontal bar chart for Kills per Match\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(pairs_kills_per_match, effect_sizes_kills_per_match, color='blue', alpha=0.8)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1.2, alpha=0.7)  # Reference line at 0\n",
    "plt.xlabel(\"Effect Size (Cliff's Delta)\", fontsize=12)\n",
    "plt.ylabel(\"Comparison Pairs\", fontsize=12)\n",
    "plt.title(\"Effect Sizes for Kills per Match\", fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.02 if width > 0 else width - 0.08,\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             f\"{width:.3f}\",\n",
    "             va='center',\n",
    "             fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create the horizontal bar chart for Kills per Win\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(pairs_kills_per_win, effect_sizes_kills_per_win, color='green', alpha=0.8)\n",
    "plt.axvline(0, color='gray', linestyle='--', linewidth=1.2, alpha=0.7)  # Reference line at 0\n",
    "plt.xlabel(\"Effect Size (Cliff's Delta)\", fontsize=12)\n",
    "plt.ylabel(\"Comparison Pairs\", fontsize=12)\n",
    "plt.title(\"Effect Sizes for Kills per Win\", fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Annotate bars\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.02 if width > 0 else width - 0.08,\n",
    "             bar.get_y() + bar.get_height() / 2,\n",
    "             f\"{width:.3f}\",\n",
    "             va='center',\n",
    "             fontsize=10)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
